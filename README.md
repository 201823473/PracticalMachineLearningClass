# Practical Machine Learning

*Instructor: Alejandro Correa Bahnsen*

- email: <al.bahnsen@gmail.com>
- twitter: [@albahnsen](https://twitter.com/albahnsen)
- github: [albahnsen](http://github.com/albahnsen)


The use of statistical models in computer algorithms allows computers to make decisions and predictions, and to perform tasks that traditionally require human cognitive abilities. Machine learning is the interdisciplinary field at the intersection of statistics and computer science which develops such algorithnms and interweaves them with computer systems. It underpins many modern technologies, such as speech recognition, internet search, bioinformatics, computer vision, Amazon’s recommender system, Google’s driverless car and the most recent imaging systems for cancer diagnosis are all based on Machine Learning technology.

This course on Machine Learning will explain how to build systems that learn and adapt using real-world applications. Some of the topics to be covered include machine learning, python data analysis, deep learning frameworks, natural language processing models and recurrent models. The course will be project-oriented, with emphasis placed on writing software implementations of learning algorithms applied to real-world problems, in particular, image analysis, image captioning, natural language pocessing, sentiment detection, among others.

## Requiriments 
* [Python](http://www.python.org) version 3.5;
* [Numpy](http://www.numpy.org), the core numerical extensions for linear algebra and multidimensional arrays;
* [Scipy](http://www.scipy.org), additional libraries for scientific programming;
* [Matplotlib](http://matplotlib.sf.net), excellent plotting and graphing libraries;
* [IPython](http://ipython.org), with the additional libraries required for the notebook interface.
* [Pandas](http://pandas.pydata.org/), Python version of R dataframe
* [Seaborn](stanford.edu/~mwaskom/software/seaborn/), used mainly for plot styling
* [scikit-learn](http://scikit-learn.org), Machine learning library!

A good, easy to install option that supports Mac, Windows, and Linux, and that has all of these packages (and much more) is the [Anaconda](https://www.continuum.io/).

GIT!! Unfortunatelly out of the scope of this class, but please take a look at these [tutorials](https://help.github.com/articles/good-resources-for-learning-git-and-github/)

## Schedule

| Date | Session         | Notebooks/Presentations          | Exercises |
| :----| :----| :------------- | :------------- | 
| January 21st | Introduction to python and ML | <ul><li>[1 - Intro to ML]() </li> <li>[2 - Intro to Python for data analysis]() </li></ul> | <ul><li>[E1 - Data Science Overview]() </li><li>[E2 - Python Text Analysis]() </li>  </ul> | 
| January 28th | Linear Models | <ul><li>[3 - Linear Regression]() </li> <li>[4 - Logistic Regression]() </li>  </ul> | <ul><li>[E3 - Supervised vs Unsupervised Overview]() </li> <li>[E4 - Linear Models]() </li> </ul> | 
| February 4th | Machine Learning Systems | <ul><li>[5 - Data preparation]() </li> <li>[6 - Model Evaluation]() </li>  <li>[7 - Model Deployment]() </li></ul> | <ul><li>[P1 - Survival Prediction API]() </li> </ul> | 
| February 11st | Naive Bayes & Decision Trees  | <ul><li>[8 - Naive Bayes]() </li><li>[9 - Decision Tress]() </li></ul> | <ul><li>[E5 - Decision Trees Overview]() </li> <li>[E6 - ]() </li></ul> | 
| February 18th |  Ensembles | <ul><li>[10 - Bagging]() </li><li>[11 - Bagging]() </li><li>[12 - Boosting]() </li></ul> | <ul><li>[E7 - Best Ensemble Overview]() </li><li>[E8 - ]() </li> </ul> | 
| February 25th |  Feature Engineering | <ul><li>[13 - Feature Engineering]() </li></ul> | <ul><li>[E9 - Importance of Feature Engineering Overview]() </li> <li>[E10 - ]() </li> </ul> | 
| March 4th | Unbalanced Datasets  | <ul><li>[14 - Unbalanced Datasets]() </li></ul> | <ul><li>[E11 - Real World Datasets Overview]() </li> <li>[E12 - Fraud Detection]() </li></ul> | 
| March 11th |  Other Supervised Methods | <ul><li>[- KNN]() </li><li>[- Support Vector Machines]() </li> <li>[- Regularization]() </li></ul> | <ul><li>[P2 - Classification API]() </li> </ul> | 
| March 18th |  Un-Supervised Learning | <ul><li>[- PCA]() </li></ul> | <ul><li>[]() </li> </ul> | 
| April 1st |  Outlier Detection | <ul><li>[ - Anomaly Detection]() </li><li>[ - Isolation Forests]() </li></ul> | <ul><li>[]() </li> </ul> | 
| April 3rd |  Information Retrieval | <ul><li>[ - Data Scrapping]() </li><li>[ - Social Networks APIs]() </li></ul> | <ul><li>[]() </li> </ul> | 
| April 8th |  Natural Language Processing | <ul><li>[]() </li></ul> | <ul><li>[P3 - NLP Cheating Detection API]() </li> </ul> | 
| April 22nd | Sentiment Analysis  | <ul><li>[]() </li></ul> | <ul><li>[]() </li> </ul> | 
| April 24th |  Hyperparameters Tuning | <ul><li>[]() </li></ul> | <ul><li>[]() </li> </ul> | 
| May 6th |  Introduction to Deep Learning | <ul><li>[]() </li></ul> | <ul><li>[]() </li> </ul> | 
| May TBD | Final Project Presentation  | <ul><li>[]() </li></ul> | <ul><li>[P4 - Kaggle Competition]() </li> </ul> | 

